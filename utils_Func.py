#!/usr/bin/env python
# coding: utf-8

# In[1]:


#import libraries
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import scipy.io as sio
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from scipy.spatial.distance import cdist
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, Ridge
from mapie.metrics import regression_coverage_score
from sklearn.metrics import mean_squared_error as MSE
from sklearn.metrics import explained_variance_score
from matplotlib.legend import _get_legend_handles_labels
import os
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"  # disable debugging logs from Tensorflow
import warnings
warnings.filterwarnings("ignore")
from sklearn.preprocessing import StandardScaler
from numpy import sqrt
from scipy.stats import randint, uniform
from matplotlib.offsetbox import TextArea, AnnotationBbox
from matplotlib.ticker import FormatStrFormatter
from sklearn.multioutput import MultiOutputRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel
from lightgbm import LGBMRegressor
from matplotlib.offsetbox import AnnotationBbox, TextArea
from matplotlib.ticker import FormatStrFormatter
from scipy.stats import randint, uniform
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split

from mapie.metrics import (regression_coverage_score,
                           regression_mean_width_score)
from mapie.quantile_regression import MapieQuantileRegressor
from mapie.regression import MapieRegressor
from mapie.subsample import Subsample


random_state = 23
rng = np.random.default_rng(random_state)
round_to = 3

warnings.filterwarnings("ignore")
plt.rc('xtick',labelsize=19)
plt.rc('ytick',labelsize=19)
plt.rc('axes', labelsize=20, titlesize=16)
# To plot consistent and pretty figures
get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib as mpl
mpl.rcParams['lines.linewidth'] = 2
mpl.rcParams['figure.dpi'] = 120
mpl.rcParams['savefig.dpi'] = 120

#Functions
# The function works by counting the number of true values in the test set that fall within the prediction 
# intervals. It then divides this count by the total number of data points to compute the coverage score, 
# which represents the percentage of true values that are covered by the prediction intervals. 
# A coverage score of 1 indicates that all true values are within the prediction intervals, while a score of 0 
# indicates that none of the true values are covered by the intervals. The function is a useful tool for 
# assessing the effectiveness of the prediction intervals generated by different methods.
def coverage_score(Ytest, PIs_low, PIs_up):
    # Initialize a count to keep track of the number of true values falling within the prediction intervals
    count = 0

    # Iterate through each data point
    for i in range(len(Ytest)):
        # Check if the true value is within the prediction interval for each output
        if Ytest[i] >= PIs_low[i] and Ytest[i] <= PIs_up[i]:
            count += 1
    
    # Calculate the coverage score by dividing the count of true values in the prediction intervals by the total number of data points
    coverage = count / len(Ytest)
    return coverage
# This is the implementation of the CV_plus function, which calculates prediction intervals using 
# cross-validation. The function takes six arguments: X (the feature matrix of the training data), 
#     Y (the target values of the training data), Xtest (the feature matrix of the test data), 
#     Ytest (the true target values of the test data), model (the regression model to use for predictions), 
#     alpha (the significance level for the prediction intervals), and an optional argument K_CV 
#     (the number of folds for cross-validation, default is 100).
def CV_plus(X,Y,Xtest,Ytest,model,alpha,K_CV=100):
    n = X.shape[0]
    m = int(n/K_CV)      
    ntest = Xtest.shape[0]
    y_pred=np.zeros((n,ntest))
    R = np.zeros(n)
    res=[]
    for k in np.arange(K_CV):
        fold = np.arange(k*m,(k+1)*m)    
        X_ = np.delete(X,fold,0)
        Y_ = np.delete(Y,fold)
        model.fit(X_, Y_)
        R[fold] = np.abs(Y[fold].reshape(-1) -model.predict(X[fold]))
        y_pred[fold]=model.predict(Xtest)
    PIs = np.zeros((ntest,2))
    for itest in np.arange(ntest):
        q_lo = np.sort(y_pred[:,itest]-R)[::-1][(np.ceil((1-alpha)*(n+1))).astype(int)]
        q_hi = np.sort(y_pred[:,itest]+R)[(np.ceil((1-alpha)*(n+1))).astype(int)]
        PIs[itest] = np.array([q_lo,q_hi])
    coverage=round(coverage_score(Ytest, PIs[:,0], PIs[:,1]),3)
    width=( PIs[:,1] - PIs[:,0]).mean().round(3)
    res.append([alpha,coverage,width,PIs])
    return res
# The sort_y_values function is designed to sort the dataset in ascending order based on the true target values
# (y_test). This sorting is performed to facilitate creating plots using the fill_between function, which
# typically requires the data points to be in a specific order for correct visualization. 
# The function takes four arguments: y_test (the true target values of the test data), 
# y_pred (the predicted target values for the test data), and y_pis (the lower and upper bounds of the 
# prediction intervals).

def sort_y_values(y_test, y_pred, y_pis):
    """
    Sorting the dataset in order to make plots using the fill_between function.
    """
    indices = np.argsort(y_test)
    y_test_sorted = np.array(y_test)[indices]
    y_pred_sorted = y_pred[indices]
    y_lower_bound = y_pis[:, 0, 0][indices]
    y_upper_bound = y_pis[:, 1, 0][indices]
    return y_test_sorted, y_pred_sorted, y_lower_bound, y_upper_bound

# The CQR function is designed to compute Conformal Quantile Regression (CQR) prediction intervals for a 
# given regression model. It takes several input arguments, including X_train (training data features), 
# y_train (training data target values), X_calib (calibration data features), y_calib (calibration data target
# values), X_test (test data features), y_test (test data target values), estimator (the base regression model
# used for prediction), and an optional alpha parameter (the significance level for the prediction intervals, 
# which defaults to 0.05).
def CQR(X_train, y_train, X_calib, y_calib, X_test, y_test, estimator, alpha=0.05):
    parameters = {
        "cqr": {"method": "quantile", "cv": "split", "alpha": alpha}
    }
    
    y_pred = {}
    y_pis = {}
    y_test_sorted = {}
    y_pred_sorted = {}
    lower_bound = {}
    upper_bound = {}
    coverage = {}
    width = {}
    
    for strategy, params in parameters.items():
        model = MapieQuantileRegressor(estimator, **params)
        model.fit(X_train, y_train, X_calib=X_calib, y_calib=y_calib)
        y_pred[strategy], y_pis[strategy] = model.predict(X_test)
        
        y_test_sorted[strategy], y_pred_sorted[strategy], lower_bound[strategy], upper_bound[strategy] = sort_y_values(y_test, y_pred[strategy], y_pis[strategy])
        
        coverage[strategy] = regression_coverage_score(y_test, y_pis[strategy][:, 0, 0], y_pis[strategy][:, 1, 0])
        width[strategy] = regression_mean_width_score(y_pis[strategy][:, 0, 0], y_pis[strategy][:, 1, 0])
    
    return y_pred, y_pis, y_test_sorted, y_pred_sorted, lower_bound, upper_bound, coverage, width


def plot_PI(
    title,
    axs,
    y_test_sorted,
    y_pred_sorted,
    lower_bound,
    upper_bound,
    coverage,
    width,
    missed,
    num_plots_idx
):
    """
    Plot of the prediction intervals for each different conformal method.
    """
    # Set the y-axis and x-axis major formatters to display integers on both axes
    axs.yaxis.set_major_formatter(FormatStrFormatter('%.0f' ))
    axs.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))

    # Extract the data points specified by 'num_plots_idx' from the arrays
    lower_bound_ = np.take(lower_bound, num_plots_idx)
    y_pred_sorted_ = np.take(y_pred_sorted, num_plots_idx)
    y_test_sorted_ = np.take(y_test_sorted, num_plots_idx)

    # Calculate the prediction interval error as the difference between predicted values and lower bounds
    error = y_pred_sorted_ - lower_bound_

    # Identify data points where the true value is outside the prediction interval (warnings)
    warning1 = y_test_sorted_ > y_pred_sorted_ + error
    warning2 = y_test_sorted_ < y_pred_sorted_ - error
    warnings = warning1 + warning2

    # Plot the data points within the prediction interval using 'errorbar'
    axs.errorbar(
        y_test_sorted_[~warnings],
        y_pred_sorted_[~warnings],
        yerr=error[~warnings],
        capsize=2, marker="o", elinewidth=1, linewidth=0, ecolor='lightgray', color='lightgray',
        label="Inside prediction interval"
    )

    # Plot the data points outside the prediction interval using 'errorbar'
    axs.errorbar(
        y_test_sorted_[warnings],
        y_pred_sorted_[warnings],
        yerr=error[warnings],
        capsize=1, marker="o", elinewidth=1, linewidth=0, color="red",
        label="Outside prediction interval"
    )

    # Plot the true target values using 'scatter' at the positions of the data points outside the prediction interval
    axs.scatter(
        y_test_sorted_[warnings],
        y_test_sorted_[warnings],
        s=40, marker="*", color="green",
        label="True value"
    )

    # Set the plot limits to ensure the plot is square and show the line x=y (the diagonal line where true values equal predicted values)
    lims = [
        np.min([axs.get_xlim(), axs.get_ylim()]),
        np.max([axs.get_xlim(), axs.get_ylim()]),
    ]
    axs.plot(lims, lims, '--', alpha=0.75, color="black", label="x=y")

    # Set the title of the plot with the method's name and its coverage, width, and the number of points missed in the prediction interval
    axs.set_title(title + f" (Coverage: {coverage}, Width: {width}, Missed: {missed})", fontweight='bold', fontsize=25)
    
    # Set the aspect ratio of the plot to be equal
    axs.set(adjustable='box', aspect='equal')

    
def widths_bins(want, y_testt, r, a_sort, names, bins):
    """
    Calculate the width or coverage score for prediction intervals in different bins of y_testt.
    """
    # Define the bin edges based on the provided 'bins'
    cuts = []
    cuts_ = pd.qcut(y_testt, bins).unique()[:-1]
    for item in cuts_:
        cuts.append(item.left)
    cuts.append(cuts_[-1].right)
    cuts.append(np.max(y_testt) + 1)

    # Create a dictionary 'recap' to store the results for each bin
    recap = {}
    for i in range(len(cuts) - 1):
        cut1, cut2 = cuts[i], cuts[i + 1]
        name = f"[{np.round(cut1, 2)}, {np.round(cut2, 2)}]"
        recap[name] = []

        # Iterate through each strategy (e.g., CV+ and CQR)
        for strategy in names:
            # Filter data points within the current bin range
            indices = np.where((y_testt >= cuts[i]) * (y_testt <= cuts[i + 1]))
            y_test_trunc = np.take(y_testt, indices)
            y_low_ = np.take(r[strategy][:, 0], indices)
            y_high_ = np.take(r[strategy][:, 1], indices)

            # Calculate coverage score and width for the prediction intervals within the bin
            score_coverage = regression_coverage_score(
                y_test_trunc[0], y_low_[0], y_high_[0]
            )
            width = (y_high_[0] - y_low_[0]).mean().round(3)

            # Append the score (coverage or width) to the corresponding bin in the 'recap' dictionary
            if want == "coverage":
                recap[name].append(score_coverage)
            elif want == "width":
                recap[name].append(width)

    # Convert the 'recap' dictionary to a DataFrame and return it
    recap_df = pd.DataFrame(recap, index=names)
    return recap_df


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:




